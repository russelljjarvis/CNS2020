{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNS Abstract\n",
    "\n",
    "In this project I used Princple Component Analysis to examine the degree of separability between modeled neuron electrical recordings and real electrical recordings from actual neurons.\n",
    "\n",
    "If biologically realistic models were better at imitating real experimental cells, then data and models would not easily be discriminable. By plotting a 48 dimensional feature space onto a two dimensional projection space, I show that a diverse pool of data and models are readily discriminated via Random Forest Classification, a result, that leaves even some of the most optimized models lacking. The idea is that the models which are the most resistant to being correctly machine-classified as models (therefore being misclassified as data), serve as better imitations/mimics of experimental data. I also used random forest regression to investigate when experimental data inform a classifying statistical model which dimensions explain the most of the observed variance in the feature space. Variance-explained will facilitate the production of a list of improvements to make to our models in order to render models better imitations of real data.\n",
    "\n",
    "In this project you can see use of:\n",
    "* PCA, t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "* Random Forest Classification (RFC) using 38 features, and also RFC applied to just 2 features (output from PCA).\n",
    "* using the RFC \"variance-explained\" feature. \n",
    "* Plotting of a decision boundary. (need to redo). \n",
    "* Not done yet, but pending Cross-Validation using looping over many different test/train splits.\n",
    "\n",
    "### Broader Project Context and Background:\n",
    "\n",
    "There is a great diversity of real biological neurons, all of which differ substantially in their electrical behavior. There are a few different classes of general purpose neuronal models, that can reproduce these different types of electrical behaviours, given appropriate parameterizations of the models.\n",
    "\n",
    "An exisiting class of neuron model type, called The Izhikevich model was published with parameter sets believed to make the model outputs accurately align with a variety of real biological cell outputs. However since publication much very specific electro physiological recordings have accumulated, that in someways undermine model/experiment agreement. However it is now possible to constrain the Izhikevich model and find new parameterizations that more allow us to more accurately reproduce more recently published experimental data.\n",
    "\n",
    "In contrast to other projects that seek to use features to seperate and classify two different categories of things that are hard to tell apart, such that humans can benefit from a fast classification of hard to discern differences in high dimensional spaces. In this project the goal is to use resistance to classification as an indicator of an optimization algorithms success, and to use machine seperation of data categories as an error signal, that directs us to precise locations of model failure. Another way of saying this, is, if a good/fair attempt at machine classification is hard, then then a different machine learning algorithm did a good job. If machine classification is very easy, the optimization algorithm did a poor job.\n",
    "\n",
    "\n",
    "### Code authorship.\n",
    "I used the approach described herein for different research work intended for a conference abstract published as follows:\n",
    "J Birgiolas, R Jarvis, V Haynes, R Gerkin, SM Crook (2019) Automated assessment and comparison of cortical neuron models BMC Neuroscience 2019, 20(Suppl 1):P47 \n",
    "\n",
    "The application of TSNE to data was developed in a research team context on different data pertaining to ion channels, or the APs exclusively derived from models (as opposed to a combination of models and data). In the context of this project, I have used novel experimental data (pulled from the Allen Brain Portal API) and novel models (8 optimized cell models included), so I have re-applied a small amount of code from pre-established work, but I have made substantial novel contributions, by looking at different features, applying different feature engineering, applying Random Forest Classification, applying variance explained, and interpreting results. For a comparison to other pre-established work that informed this work check [here](https://github.com/vrhaynes/LargeScaleModelAnalysis_2019/blob/master/model-data-clustering.ipynb)\n",
    "\n",
    "### Model Optimization as a data pre-processing stage.\n",
    "Before Machine Learning and analysis techniques could be applied, we needed to find optimized models. These optimized models can be understood as models that are intended to be superior mimics of real biologically derived data, as their governing equation parameters have been more rigorously constrained  by a wider range of experimental data.\n",
    "\n",
    "In order illustrate that the optimized models are better imitations of real data, four adaptive Exponential models, and four Izhikevich models each were fitted to four different classes of experimental cells see implementation in ipython notebook [Notebook](https://github.com/russelljjarvis/neuronunit/blob/master/neuronunit/examples/seperate_out_data_new_models.ipynb). These eight fitted models were subsequently fed into a Druckman feature extraction algorithm, and added as data points in a dimension reduced plot of the feature space. Many pre-existing neural models, and some Allen Brain Data where also plotted as contextual data in the same feature space.\n",
    "\n",
    "## Project Implementation and Technologies\n",
    "\n",
    "* Python, pandas sklearn, dask were all used for Model Optimization pre-processing steps, and for plotting the models in a dimension reduced feature space.\n",
    "* Models versus Data. Models which are resistant to being classified as models are more successful, and better representatives of data. See below.\n",
    "* The optimized cells were derived from a custom built parallel genetic algorithm, utilizing pre-existing python tools: ***DEAP*** and ***Dask***. It would have been desirable to optimize the models with an algorithm from this course, such as  Lasso, ridge regression, and elastic search (L1+L2)/2 regularization combined. The way I do this is to run a genetic algorithm over the data, The genetic algorithm is performing its own type of guided sparse sampling of the data.\n",
    "\n",
    "The Druckman feature analysis protocol originates from MATLAB code associated with the analysis of Blue Brain Project Modelled cells, this feature analysis pipeline was then ported to Python by Justas Birgiolas, at a later point I made the feature analysis pipeline applicable to optimized Adaptive Exponential and Izhiketch cells. Rick Gerkin and Vergil Haynes, assisted in data cleaning preperation and TSNE application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install update matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pickle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below come from Efeatures:\n",
    "https://efel.readthedocs.io/en/latest/eFeatures.html#spike-shape-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the figures below you can some different electrical behavior corresponding to two different multi-spiking electrical experiments.\n",
    "![spiking.png](spiking.png)\n",
    "![timing_and_after_spike_depth](timing_and_after_spike_depth.png)\n",
    "\n",
    "The following figure shows the difference in a multispiking waveforms between an Izhikitich model and an adaptive exponential spiking model:\n",
    "\n",
    "![electrical_behavior.png](electrical_behavior.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe transformed values, ordered from highest to lowest variance dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "#![sag.png](sag.png)\n",
    "import plotly.io as pio\n",
    "GITHUB = True\n",
    "if GITHUB:\n",
    "    pio.renderers.default = \"svg\"\n",
    "else:\n",
    "    pio.renderers.default = 'notebook'\n",
    "\n",
    "#![passive_properties.png](passive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "data = []\n",
    "import warnings\n",
    "#warnings.filter(\"ignore\")\n",
    "import pickle\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import seaborn\n",
    "seaborn.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "#!conda install -c plotly plotly-orca\n",
    "useable = pickle.load(open('optimized_multi_feature','rb'))\n",
    "useable[0].tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "#pd.set_option('max_columns',None)\n",
    "#pd.set_option('max_rows',None)\n",
    "\n",
    "hbp = pickle.load(open(\"hbp_data.p\",\"rb\"))\n",
    "df_o_m3 = None\n",
    "dict_ = {}\n",
    "\n",
    "for i in hbp:\n",
    "    #for lit in list(i.out_dic.items()):\n",
    "    if i is not None:\n",
    "        #mod = rekeyeddm(mod)\n",
    "        for v in list(i.out_dic.values()):\n",
    "            temp = list(i.out_dic.values())\n",
    "            if temp[1] is not None:\n",
    "                dict_.update(temp[1][0])\n",
    "                #print(temp[0])\n",
    "        data = pd.DataFrame(data=dict_,index=[temp[0]])\n",
    "        if i == 0:\n",
    "            df_o_m3 = data\n",
    "        else:\n",
    "            df_o_m3 = pd.concat([data,df_o_m3])\n",
    "            \n",
    "del df_o_m3['interburst_voltage']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m3 = pd.DataFrame.drop_duplicates(df_o_m3)\n",
    "bbp_frame = df_o_m3\n",
    "stable_list = list(bbp_frame.index.values);\n",
    "\n",
    "df_o_m3;\n",
    "stable_list;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.concat([df,bbp_frame])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import dask.dataframe as dd    \n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set(font_scale=1.5)\n",
    "import pandas as pd\n",
    "os.getcwd()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "from neuronunit.optimisation.optimization_management import three_step_protocol, filtered\n",
    "\n",
    "# Special stuff to import\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load optimized reduced cell models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "with open('dm_on_models.p','rb') as f:\n",
    "    (RAW_dtc,ADEXP_dtc) = pickle.load(f)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimized cells through 3 different third party feature extraction routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "try:\n",
    "    useable = pickle.load(open('optimized_multi_feature','rb'))\n",
    "\n",
    "except:\n",
    "    useable = []\n",
    "\n",
    "    for value in RAW_dtc.values():\n",
    "        dtcpop = value\n",
    "        dtcpop = [ dtc for dtc in dtcpop if type(dtc.rheobase) is not type(None) ]\n",
    "        useable.extend(list(map(feature_mine,dtcpop)))\n",
    "\n",
    "    useable = [ dtc for dtc in dtcpop if hasattr(dtc,'allen_30') ]\n",
    "    pickle.dump(useable,open('optimized_multi_feature','wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate 3 different feature sets into a unified and aligned data frame.\n",
    "\n",
    "For five optimized cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m2 = None\n",
    "dict_ = {}\n",
    "\n",
    "for i,dtc in enumerate(useable):\n",
    "    dict_.update(**dtc.everything)\n",
    "    dict_.update(**dtc.dm_results_regular)\n",
    "    data = pd.DataFrame([dict_])\n",
    "    if i == 0:\n",
    "        df_o_m2 = data\n",
    "    else:\n",
    "        df_o_m2 = pd.concat([data,df_o_m2])\n",
    "\n",
    "df_o_m = df_o_m2\n",
    "df_o_m;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the blue brain data with the naan column may be what breaks other versions of this notebook.\n",
    "\n",
    "Its data frame is called `df_o_m3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # A function to convert all cells containing array (or other things) into floats.  \n",
    "def f(x):\n",
    "    try:\n",
    "        return np.mean(x)\n",
    "    except:\n",
    "        try:\n",
    "            return np.mean(x['mean'])\n",
    "        except:\n",
    "            print(x)\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we have a data frame just for optimized cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load wrangle and clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the 1.5x rheobase file\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "path2data = os.path.join(cwd,'data')\n",
    "filename = os.path.join(cwd,'onefive_df.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "    # A function to convert all cells containing array (or other things) into floats.  \n",
    "def custom_cleaner(x):\n",
    "    try:\n",
    "        return np.mean(x)\n",
    "    except:\n",
    "        try:\n",
    "            return np.mean(x['pred'])\n",
    "        except:\n",
    "            return np.mean(x['mean'])\n",
    "df = df.fillna(0).applymap(custom_cleaner)\n",
    "\n",
    "# Apply this function to each dataframe in order to convert all cells into floats.\n",
    "# Also call fillna() first to impute missing values with 0's.  \n",
    "%time df = df.fillna(0).applymap(custom_cleaner)\n",
    "#df_30x = df_30x.fillna(0).applymap(f)\n",
    "df.head();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_frame_columns(df_everything,modifyer):\n",
    "    \"\"\"\n",
    "    Adjust data frame column names so they fit into a universal scheme.\n",
    "    \"\"\"\n",
    "    for c in df_everything.columns:\n",
    "        if str(c)+str('_1.5x') in modifyer.columns:\n",
    "            df_everything[str(c)+str('_1.5x')] = df_everything[c]\n",
    "        if str(c)+str('_3.0x') in modifyer.columns:\n",
    "            df_everything[str(c)+str('_3.0x')] = df_everything[c]\n",
    "    temp = df_everything\n",
    "    for c in df_everything.columns:\n",
    "        if str('_1.5x') not in str(c) and str('_3.0x') not in str(c): #in df_everything.columns:\n",
    "            temp = temp.drop(columns=[c])\n",
    "        #if str(c)+str('_3.0x') in df_everything.columns:\n",
    "        #    df_everything[str(c)+str('_1.5x')] = df_everything[c]\n",
    "    df_everything = temp\n",
    "    return df_everything\n",
    "\n",
    "\n",
    "df_o_m3 = mutate_frame_columns(df_o_m3,df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m3.tail(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_o_m3])\n",
    "df['AP_fall_indices_1.5x'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)#.applymap(f)\n",
    "df = df.applymap(custom_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbp_frame = pd.DataFrame.drop_duplicates(bbp_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[42]\n",
    "df_everything = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://celltypes.brain-map.org/experiment/morphology/313862167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_everything = df_everything.fillna(0)\n",
    "df_everything;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_everything.tail(15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare a dictionary \n",
    "Which serves as a table helping us to align Druckman features with high current and low current protocols in other feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_reduncy(df,removed):\n",
    "    for c in removed.columns:\n",
    "        if c in df.columns:\n",
    "            del df[c]\n",
    "    return df\n",
    "\n",
    "with open('redundancy_removed.p','rb') as f:\n",
    "    removed = pickle.load(f)\n",
    "    \n",
    "df = remove_reduncy(df,removed)\n",
    "\n",
    "for c in removed.columns:\n",
    "    try:\n",
    "        del df[c]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0).applymap(custom_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have prepared a pandas data frame, where the first half of the data frame, that is the first ***448*** entries are Druckman measurements pertaining to voltage traces recorded in real biological cells. Appended immediately below in the same data frame we have ***965*** Druckman measurements derived from NeuroML models. A print out of this frame follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify of the features that explained most variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the features that explain the most variance.\n",
    "\n",
    "\n",
    "# delete features \n",
    "which were not available or accessible in the real data. This \n",
    "includes mainly negative current injection protocols such as input resistance etc.\n",
    "\n",
    "if 'InputResistanceTest_1.5x' in df.columns:\n",
    "    # in order to find out what is seperating and what is not.\n",
    "    del df['InputResistanceTest_1.5x']\n",
    "    del df['InputResistanceTest_3.0x']\n",
    "    del df['ohmic_input_resistance_1.5x']\n",
    "    del df['ohmic_input_resistance_3.0x']\n",
    "    del df['time_1.5x']                              \n",
    "    #       0.190362\n",
    "    del df['decay_time_constant_after_stim_3.0x']\n",
    "    del df['voltage_deflection_3.0x']\n",
    "    del df['steady_state_hyper_3.0x']\n",
    "    del df['steady_state_voltage_stimend_3.0x']\n",
    "    del df['voltage_deflection_vb_ssse_3.0x']\n",
    "    del df['sag_amplitude_3.0x']\n",
    "    #0.198310\n",
    "    del df['is_not_stuck_1.5x']\n",
    "    del df['AHP_depth_abs_1.5x']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes horizonstally, so that all features coming from df_15x get a '_1.5x' suffix\n",
    "# and all the ones from df_30x get a '_3.0x' suffix\n",
    "#df = df_15x.join(df_30x, lsuffix='_1.5x', rsuffix='_3.0x')\n",
    "def other_cleaner(x):\n",
    "    if type(x) is type(dict()):\n",
    "        try:\n",
    "            return np.mean(x['pred'])\n",
    "        except:\n",
    "            return np.mean(x['mean'])\n",
    "\n",
    "    else:\n",
    "        return np.mean(x)\n",
    "   \n",
    "df = df_everything.fillna(0).applymap(other_cleaner)\n",
    "df_everything = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %d models+data and %d features\" % df_everything.shape)\n",
    "#print(\"There are %d models+data and %d features\" % dask_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data frame is big.\n",
    "*lets experiment with dask -lazy pandas array to avoid storing all the data frame in memory all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all features into Normal(0,1) variables\n",
    "# Important since features all have different scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make model dataframe\n",
    "\n",
    "model_idx2 = df.head(7).index.tolist()#  list(range(0,9))#idx for idx in df.index.values if type(idx)==str]\n",
    "model_no_trans_df2 = df[df.index.isin(model_idx2)]\n",
    "model_no_trans_df2.index.name = 'OptCells'\n",
    "model_df2 = model_no_trans_df2.copy()\n",
    "model_df2.index.name = 'OptCells'\n",
    "#model_df2[:][\"type\"] = \"opt_models\"\n",
    "model_idx = [idx for idx in df.index.values if type(idx)==str]\n",
    "model_no_trans_df = df[df.index.isin(model_idx)]\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "experiment_no_trans_df = df[df.index.isin(experiment_idx)]\n",
    "experiment_df = experiment_no_trans_df.copy()\n",
    "\n",
    "model_df2\n",
    "model_no_trans_df2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = pd.DataFrame.drop_duplicates(experiment_df)\n",
    "experiment_df.groupby(experiment_df.index).first()\n",
    "len(experiment_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame.drop_duplicates(model_df)\n",
    "model_df.index\n",
    "model_df.groupby(model_df.index).first()\n",
    "not_empty = copy.copy(df)\n",
    "not_empty;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select out models\n",
    "that we know are _intended_ to be highly representative of the data.\n",
    "These cells models are by author Gouwens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary visual area layer 4 spiny 479728896 Cell\n",
    "https://neuroml-db.org/model_info?model_id=NMLCL001508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the cells below.\n",
    "I use PCA as a heuristic aid, to facilitate human intuition about Machine classification of data versus models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dimensionality reduction assists human visual system classification\n",
    "\n",
    "It is possible for the human visual system to see three clusters of cell data points.\n",
    "\n",
    "Let's show that we can classify in this low dimensional space (by just using two features). We will slowly build up to classification via first applying Kmeans, to visualize cluster centres.\n",
    "And then move on to using a random forest approach to actually visualizing decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make experiment dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_reduncy(df,removed):\n",
    "    for c in removed.columns:\n",
    "        if c in df.columns:\n",
    "            del df[c]\n",
    "    return df\n",
    "\n",
    "with open('redundancy_removed.p','rb') as f:\n",
    "    removed = pickle.load(f)\n",
    "    \n",
    "df = remove_reduncy(df,removed)\n",
    "print(len(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does variance look like in just the data?\n",
    "## Lets examine Variance in the different Data sources alone.\n",
    "\n",
    "We can answer the question are electrophyiological measurements in cells from different experimental sources discriminible.\n",
    "\n",
    "Actually there is good agreement in the variance between Blue Brain Cell Data and Allen Brain Cell data.\n",
    "\n",
    "Adding in a variety of models could change this story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m3.index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m3 = df_o_m3.replace([np.inf, -np.inf], np.nan)#.dropna()\n",
    "experiment_df = experiment_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df_data = pd.concat([experiment_df,df_o_m3])\n",
    "\n",
    "df1 = copy.copy(df)\n",
    "\n",
    "for c in df1.columns:\n",
    "    if np.mean(df1[c])==0 or np.var(df1[c])==0:\n",
    "        print(c)\n",
    "        \n",
    "df.apply(custom_cleaner);\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate models and data by color code\n",
    "\n",
    "# break columns into chunks\n",
    "\n",
    "# more horizontal space between plots \n",
    "\n",
    "# logically reorganize\n",
    "\n",
    "\n",
    "# play around with KDE plot smoothing kernel cutoffs get rid of outliers with cutoff\n",
    "\n",
    "\n",
    "# convert 600dpi pdf\n",
    "\n",
    "bw bandwidth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_index_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('precision', 3)\n",
    "n_cols = 3\n",
    "n_rows = int(len(df1.columns)/3)+1\n",
    "fig, axes = plt.subplots(n_rows,3,figsize=(15, 3*n_rows))\n",
    "for i, feature in enumerate(df1.columns):\n",
    "    ax = axes.flat[i]\n",
    "    ax.set_ylabel(str(feature)[0:10])\n",
    "    #df1[feature].hist(alpha=0.3, ax=ax)\n",
    "    n = df1.loc[model_index_labels, feature].notnull().sum()\n",
    "    sns.kdeplot(df1.loc[model_index_labels, feature], color='r', ax=ax, label=('%d' % n))\n",
    "    sns.kdeplot(df1.loc[experiment_idx_labels, feature], color='b', ax=ax)\n",
    "    #ax.legend()\n",
    "    ax.set_ylabel(feature.replace('_', '\\n'))\n",
    "    #ax.set_xlabel(units[feature])\n",
    "#plt.tight_layout()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(len(df1.columns)/3)+1\n",
    "fig, axes = plt.subplots(n_rows,3,figsize=(15, 3*n_rows))\n",
    "for i, feature in enumerate(df1.columns):\n",
    "    ax = axes.flat[i]\n",
    "    ax.set_ylabel(str(feature)[0:10])\n",
    "    #df1[feature].hist(alpha=0.3, ax=ax)\n",
    "    n = df1.loc[model_index_labels, feature].notnull().sum()\n",
    "    sns.kdeplot(df1.loc[model_index_labels, feature], color='r', ax=ax, label=('%d' % n))\n",
    "    sns.kdeplot(df1.loc[experiment_idx_labels, feature], color='b', ax=ax)\n",
    "    #ax.legend()\n",
    "    ax.set_ylabel(feature.replace('_', '\\n'))\n",
    "\n",
    "fig, axes = plt.subplots(int(np.sqrt(len(df1.columns))), int(np.sqrt(len(df1.columns)))+1,figsize=(15, 15))\n",
    "for i, feature in enumerate(df1.columns):\n",
    "    axes.flat[i].set_ylabel(str(feature)[0:10])\n",
    "    min_ = np.min(df1[feature])\n",
    "    max_ = np.max(df1[feature])\n",
    "    axes.flat[i].set_xlim(min_,max_)\n",
    "    df1[feature].plot(alpha=0.3, ax=axes.flat[i])\n",
    "plt.tight_layout()    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns;\n",
    "ss = StandardScaler()\n",
    "df[:] = ss.fit_transform(df.values)\n",
    "df.groupby(df.index).first()\n",
    "df = pd.DataFrame.drop_duplicates(df)\n",
    "\n",
    "df[:] = preprocessing.normalize(df.values, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gouwens.p','rb') as f:\n",
    "    gouwens = pickle.load(f)\n",
    "gwn_check = list(gouwens.values())\n",
    "gwindex = gwn_check[0][0]\n",
    "gouwens_idx = [idx for idx in df.index.values if idx in gwindex]\n",
    "gouwens_idx_labels = [i for i in gouwens_idx]\n",
    "gouwens_idx_labels = df.index.isin(gouwens_idx)\n",
    "gouwens_cells = df[df.index.isin(gouwens_idx)]\n",
    "\n",
    "unusual = df[df.index.isin(gouwens_idx)]\n",
    "unusual\n",
    "print(gouwens_cells.index[-7],'a Gouwens model that well represents data')\n",
    "gouwens_cells;\n",
    "with open('markram.p','rb') as f:\n",
    "    markram = pickle.load(f)\n",
    "markram_check = list(markram.values())\n",
    "markramindex = markram_check[0][0]\n",
    "markram_idx = [idx for idx in df.index.values if idx in markramindex]\n",
    "markram_idx_labels = [i for i in markram_idx]\n",
    "markram_idx_labels = df.index.isin(markram_idx)\n",
    "markram_cells = df[df.index.isin(markram_idx)]\n",
    "\n",
    "markram_cells;\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "model_no_trans_df = df[~df.index.isin(experiment_idx)]\n",
    "experiment_idx_labels = [(i,idx) for i,idx in enumerate(df.index.values) if type(idx)==int]\n",
    "\n",
    "#model_df\n",
    "#df.labels\n",
    "model_no_trans_df\n",
    "experiment_idx_labels = [i[0] for i in experiment_idx_labels]\n",
    "experiment_idx_labels\n",
    "model_no_trans_df\n",
    "model_index_labels = ~df.index.isin(experiment_idx)\n",
    "\n",
    "model_index_labels\n",
    "\n",
    "\n",
    "new_models_idx = df.head(7).index.tolist()#  list(range(0,9))#idx for idx in df.index.values if type(idx)==str]\n",
    "new_model_labels= df.index.isin(new_models_idx)\n",
    "#len(new_models_idx)\n",
    "nm = df.head(7)\n",
    "nm;\n",
    "bbp_labels = df.index.isin(stable_list)\n",
    "bbpindex = df.index.isin(stable_list)\n",
    "bbpindex = [i for i,idx in enumerate(df.index.values) if idx in stable_list]\n",
    "bbp = []\n",
    "for idx in df.index.values:\n",
    "    if idx in stable_list:\n",
    "        bbp.append(True)\n",
    "    else:\n",
    "        bbp.append(False)\n",
    "gouwens_idx_labels\n",
    "bbp_idx = [idx for idx in df.index.values if idx in stable_list]\n",
    "bbp_idx_labels = [i for i in bbp_idx]\n",
    "bbp_idx_labels = df.index.isin(bbp_idx_labels)\n",
    "\n",
    "\n",
    "model_idx;\n",
    "model_idx = [idx for idx in df.index.values if type(idx)==str and idx not in bbp_idx]\n",
    "\n",
    "model_index_labels = df.index.isin(model_idx)\n",
    "experiment_idx_labels = df.index.isin(experiment_idx)\n",
    "bbp_idx_labels = [idx for idx in df.index.values if type(idx)==str and idx and idx in bbp_idx]\n",
    "\n",
    "\n",
    "bbp_idx_relabels = [(i,idx) for i,idx in enumerate(df.index.values) if idx in bbp_idx]\n",
    "bbp_labels = df.index.isin(stable_list)\n",
    "bbpindex = df.index.isin(stable_list)#.get_loc()\n",
    "\n",
    "df[bbpindex].index.values;\n",
    "\n",
    "\n",
    "with open('traub.p','rb') as f:\n",
    "    traub = pickle.load(f)\n",
    "traub_check = list(traub.values())\n",
    "traubindex = traub_check[0][0]\n",
    "\n",
    "traub_idx = [idx for idx in df.index.values if idx in traubindex]\n",
    "traub_idx_labels = df.index.isin(traub_idx)\n",
    "traub_cells = df[df.index.isin(traub_idx)]\n",
    "traub_cells;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bbp_idx_labels;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pd.set_option('precision', 3)\n",
    "n_cols = 3\n",
    "n_rows = int(len(df.columns)/3)+1\n",
    "fig, axes = plt.subplots(n_rows,3,figsize=(15, 3*n_rows))\n",
    "for i, feature in enumerate(df.columns):\n",
    "    ax = axes.flat[i]\n",
    "    ax.set_ylabel(str(feature)[0:10])\n",
    "    #df1[feature].hist(alpha=0.3, ax=ax)\n",
    "    n = df.loc[model_index_labels, feature].notnull().sum()\n",
    "    sns.kdeplot(df.loc[model_index_labels, feature], color='r', ax=ax, label=('%d' % n))\n",
    "    sns.kdeplot(df.loc[experiment_idx_labels, feature], color='b', ax=ax)\n",
    "    #ax.legend()\n",
    "    ax.set_ylabel(feature.replace('_', '\\n'))\n",
    "    #ax.set_xlabel(units[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the slide below:\n",
    "You can see a plot of the high dimensional Druckman feature space projected into a low dimensional space using rotation matrices found via \n",
    "a regular PCA algorithm (not T distributed stochastic neighbourhood embedding). \n",
    "\n",
    "PCA uses rotated covariance matrices to project original data into the directions where variance in the data is maximum. One disadvantage of this approach is that two of the highest weighted eigenvalues yield synthetic dimensions, synthetic dimensions that are hard to relate back to a just a few of the original Druckman dimensions. \n",
    "\n",
    "In this way PCA and TSNE are useful data exploration tools, by the may not always lead to a complete understanding of the data.\n",
    "\n",
    "In order to circumvent this problem we will use the variance-explained feature of \"Random Forest\" classification algorithm. Random Forest variance explained, will probably hint at which dimensions comprize the greatest eigenvalues/weights of the PCA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "isomap.fit(copy.copy(df.values))\n",
    "iso = isomap.embedding_.T\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(df.values)\n",
    "n_features = df.shape[1]\n",
    "transformed = pca.transform(copy.copy(df.values))\n",
    "\n",
    "\n",
    "# Do PCA and look at variance explained\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(range(1,n_features+1),pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlim(0,50);\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Fraction of variance explained');\n",
    "df;\n",
    "\n",
    "#iso = iso.T\n",
    "#iso[0,experiment_idx_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(iso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iso = iso.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "\n",
    "plt.scatter(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],c='green',cmap='rainbow',label='data')\n",
    "plt.scatter(iso[0,model_index_labels],iso[1,model_index_labels],alpha=0.5,c='red',cmap='rainbow',label='models')\n",
    "plt.scatter(iso[0,new_model_labels],iso[1,new_model_labels],c='orange',cmap='rainbow',label='optimized/revised models')\n",
    "\n",
    "plt.scatter(iso[0,experiment_idx_labels][42],iso[1,experiment_idx_labels][42],s=700,c='yellow', alpha=0.3,cmap='rainbow',label='Real Data Layer 4 aspiny 313862167')\n",
    "plt.scatter(iso[0,gouwens_idx_labels][-7],iso[1,gouwens_idx_labels][-7],s=700,c='purple', alpha=0.3,cmap='rainbow',label='Model layer 4 spiny 479728896')\n",
    "plt.scatter(iso[0,markram_idx_labels],iso[1,markram_idx_labels],s=50,c='cyan',cmap='rainbow',label='Markram models')\n",
    "\n",
    "\n",
    "plt.scatter(iso[0,gouwens_idx_labels],iso[1,gouwens_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "#plt.scatter(iso[0,regular_iz_idx_labels],iso[1,regular_iz_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "plt.scatter(iso[0,bbp],iso[1,bbp],s=50, c='purple',cmap='rainbow',label='BBP')\n",
    "\n",
    "#plt.scatter(iso[0,:],iso[1,:],c='green',cmap='rainbow',label='data')\n",
    "\n",
    "legend = ax.legend()#handles, labels, loc=\"upper right\", title=\"Sizes\")\n",
    "# I don't love the isomap fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.clustermap(data=iso.corr)\n",
    "\n",
    "\n",
    "\n",
    "#clusty = pd.DataFrame()\n",
    "#clusty.cluster_gram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brain regions associated with NeuroML models\n",
    "\n",
    "# make four regions for ground truth\n",
    "for a region dependent classification with four regions (0,1,2,3)\n",
    "\n",
    "Gouwens and Allen 0\n",
    "Markram and BBP 1\n",
    "Hippocampus 2\n",
    "Thalamocortical 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crawl_ids(url):\n",
    "    ''' move to aibs '''\n",
    "    all_data = requests.get(url)\n",
    "    all_data = json.loads(all_data.text)\n",
    "    Model_IDs = []\n",
    "    for d in all_data:\n",
    "        Model_ID = str(d['Model_ID'])\n",
    "        Model_IDs.append(Model_ID)\n",
    "    return Model_IDs\n",
    "\n",
    "import glob \n",
    "import pickle\n",
    "import json \n",
    "import requests\n",
    "#import get_three_feature_sets_from_nml_db as runnable_nml\n",
    "#from get_three_feature_sets_from_nml_db import analyze_models_from_cache\n",
    "#from neuronunit.get_three_feature_sets_from_nml_db import crawl_ids\n",
    "def download_intensives():\n",
    "\n",
    "\n",
    "    list_to_get =[ str('https://neuroml-db.org/api/search?q=traub'),\n",
    "        str('https://neuroml-db.org/api/search?q=markram'),\n",
    "        str('https://neuroml-db.org/api/search?q=Gouwens') ]\n",
    "    regions = {}\n",
    "    for url in list_to_get:\n",
    "        Model_IDs = crawl_ids(url)\n",
    "        for Model_ID in Model_IDs:\n",
    "            url = str(\"https://neuroml-db.org/api/model?id=\")+Model_ID\n",
    "            try:            \n",
    "                model_contents = requests.get(url)\n",
    "                model_contents = json.loads(model_contents.text)\n",
    "                regions[Model_ID] = model_contents['keywords'][0]['Other_Keyword_term']\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            #print(regions)\n",
    "            with open('regions.p','wb') as f:\n",
    "                pickle.dump(regions,f)\n",
    "    return regions\n",
    "                \n",
    "regions = download_intensives()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in regions.items(): print(k,v.split(',')[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the above process not deeply informative.\n",
    "It looks like the regions are thalamo cortical (Traub)\n",
    "the regions are Somatosensory (Markram)\n",
    "the labels belong to regions are Gouwens IV (Mus Musculus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard Normalizer\n",
    "\n",
    "est = KMeans(n_clusters=3)\n",
    "est.fit(iso.T)\n",
    "y_kmeans = est.predict(iso.T)\n",
    "centers = est.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another plot but with Kmeans cluster centers included.\n",
    "Showing the cluster centres is a first step towards showing that machine classification on the dimension reduced version of the Druckman data feature space.\n",
    "\n",
    "In the plot below the two large yellow dots are the cluster centres for ***(left models),*** ***(right data)***. The Euclidian distnace from each data point from a cluster centre is directly proportional too which category the data point is from (ie model or data, ie red/blue). This visualization would assist us to understand using KMeans nearist neighbours classification algorithm to classify the data.\n",
    "\n",
    "\n",
    "IN a Random Forest Classification Analysis performed much further below we  examine the dimension that contributes the most to cluster seperation by looking at variance explained. This gives us an educated guess about dimensions that contribute the most weight to the axis of the PCA projection spaces plotted above.\n",
    "\n",
    "It is likely that the axis in the PCA plot below, are strongly aligned with \"Input Resistance\" in models and data, as well as \"AP2RateOfChangePeakToTroughTest\". This second dimension means considering multi-spiking waveforms observed in models and data, at the second Action Potential/Spike, how rapid is the decay from peak to trough of the second AP wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.scatter(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],c='green',cmap='rainbow',label='Allen Brain Experimental Data')\n",
    "plt.scatter(iso[0,model_index_labels],iso[1,model_index_labels],c='red',cmap='rainbow',label='models')\n",
    "plt.scatter(iso[0,new_model_labels],iso[1,new_model_labels],c='orange',cmap='rainbow',label='optimized/revised models')\n",
    "plt.scatter(iso[0,gouwens_idx_labels],iso[1,gouwens_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "plt.scatter(iso[0,markram_idx_labels],iso[1,markram_idx_labels],c='cyan',cmap='rainbow',label='Markram models')\n",
    "plt.scatter(iso[0,bbp],iso[1,bbp],s=90, c='purple',cmap='rainbow',label='BBP')\n",
    "#plt.scatter(iso[0,:],iso[1,:],s=10, c='purple',cmap='rainbow',label='BBP')\n",
    "\n",
    "plt.scatter(centers[0][0],centers[0][1],s=7000,c='blue', alpha=0.3, edgecolors='blue')#,label='cluster 1')\n",
    "plt.scatter(centers[1][0],centers[1][1],s=7000,c='blue', alpha=0.3,edgecolors='blue')#,label='cluster 2')\n",
    "plt.scatter(centers[2][0],centers[2][1],s=7000,c='blue', alpha=0.3,edgecolors='blue')#,label='cluster 3')\n",
    "#plt.scatter(centers[3][0],centers[3][1],s=7000,c='blue', alpha=0.3,edgecolors='blue')#,label='cluster 3')\n",
    "\n",
    "legend = ax.legend()#handles, labels, loc=\"upper right\", title=\"Sizes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace7=([iso[0,bbp]],[iso[1,bbp]],'BBP',bbp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "trace0=(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],'Allen Brain Experimental Data',experiment_idx)\n",
    "trace1=(iso[0,model_index_labels],iso[1,model_index_labels],'models',model_idx)\n",
    "trace2=(iso[0,new_model_labels],iso[1,new_model_labels],'optimized models',new_models_idx)\n",
    "trace3=(iso[0,gouwens_idx_labels],iso[1,gouwens_idx_labels],'Gouwens models',gouwens_idx)\n",
    "trace4=(iso[0,markram_idx_labels],iso[1,markram_idx_labels],'Markram models',markram_idx)\n",
    "\n",
    "trace5=([iso[0,experiment_idx_labels][42]],[iso[1,experiment_idx_labels]],'Layer 4 aspiny 313862167',experiment_idx_labels)\n",
    "trace6=([iso[0,gouwens_idx_labels][-7]],[iso[1,gouwens_idx_labels][-7]],'Layer 4 spiny 479728896',gouwens_idx_labels)\n",
    "#plt.scatter(iso[0,regular_iz_idx_labels],iso[1,regular_iz_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "#trace7=([iso[0,regular_iz_idx_labels]],[iso[1,regular_iz_idx_labels]],'regular_iz_idx_labels',regular_iz_idx_labels)\n",
    "#trace7=([iso[0,bbp]],[iso[1,bbp]],'BBP',df[bbpindex].index.values)\n",
    "trace7=(iso[0,bbp],iso[1,bbp],'Blue Brain Project Ephys Data',df[bbpindex].index.values)\n",
    "trace8=(iso[0,traub_idx_labels],iso[1,traub_idx_labels],'Traub',traub_idx_labels)\n",
    "\n",
    "#plt.scatter(iso[0,bbp_labels],iso[1,bbp_labels],s=10, c='purple',cmap='rainbow',label='BBP')\n",
    "\n",
    "traces = [trace0,trace1,trace2,trace3,trace4,trace5,trace6,trace7]#,trace8]\n",
    "cnt=0\n",
    "theme = px.colors.diverging.Portland\n",
    "\n",
    "for i,ttt in enumerate(traces):\n",
    "    if cnt==len(theme):\n",
    "        cnt=0\n",
    "    if i>1:\n",
    "        size = 12\n",
    "    else:\n",
    "        size = 6\n",
    "        #if type(ttt[3]) is not type(str()):\n",
    "        \n",
    "  \n",
    "    trace = dict(\n",
    "        type='scatter',\n",
    "        text = df[df.index.isin(ttt[3])].index,\n",
    "        x=ttt[0],\n",
    "        y=ttt[1],\n",
    "        mode='markers',\n",
    "        name=ttt[2],\n",
    "        marker=dict(\n",
    "            color=theme[cnt],\n",
    "            size=size,\n",
    "            line=dict(\n",
    "                color='rgba(217, 217, 217, 0.14)',\n",
    "                width=0.5),\n",
    "            opacity=0.8)\n",
    "    )\n",
    "    data.append(trace)\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout_title_text=\"steady_state_voltage_stimend_3.0x versus AP1DelayMeanTest_3.0x\"\n",
    ")#,\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=1050\n",
    ")\n",
    "\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    pio.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = []\n",
    "trace0=(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],'Allen Brain Experimental Data',experiment_idx)\n",
    "trace1=(iso[0,model_index_labels],iso[1,model_index_labels],'models',model_idx)\n",
    "trace2=(iso[0,new_model_labels],iso[1,new_model_labels],'optimized models',new_models_idx)\n",
    "trace3=(iso[0,gouwens_idx_labels],iso[1,gouwens_idx_labels],'Gouwens models',gouwens_idx)\n",
    "trace4=(iso[0,markram_idx_labels],iso[1,markram_idx_labels],'Markram models',markram_idx)\n",
    "\n",
    "trace5=([iso[0,experiment_idx_labels][42]],[iso[1,experiment_idx_labels]],'Layer 4 aspiny 313862167',experiment_idx_labels)\n",
    "trace6=([iso[0,gouwens_idx_labels][-7]],[iso[1,gouwens_idx_labels][-7]],'Layer 4 spiny 479728896',gouwens_idx_labels)\n",
    "#plt.scatter(iso[0,regular_iz_idx_labels],iso[1,regular_iz_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "#trace7=([iso[0,regular_iz_idx_labels]],[iso[1,regular_iz_idx_labels]],'regular_iz_idx_labels',regular_iz_idx_labels)\n",
    "#trace7=([iso[0,bbp]],[iso[1,bbp]],'BBP',df[bbpindex].index.values)\n",
    "trace7=(iso[0,bbp],iso[1,bbp],'Blue Brain Project Ephys Data',df[bbpindex].index.values)\n",
    "#trace7=(iso[0,bbp],iso[1,bbp],'Blue Brain Project Ephys Data',df[bbpindex].index.values)\n",
    "\n",
    "#plt.scatter(iso[0,bbp_labels],iso[1,bbp_labels],s=10, c='purple',cmap='rainbow',label='BBP')\n",
    "\n",
    "traces = [trace0,trace1,trace2,trace3,trace4,trace5,trace6,trace7,None]\n",
    "\n",
    "groundtruth = np.array(df.index.isin(experiment_idx))\n",
    "\n",
    "\n",
    "classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "classif.fit(iso.T, groundtruth)\n",
    "min_x = np.min(iso[0,:])\n",
    "max_x = np.max(iso[0,:])\n",
    "w = classif.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(min_x, max_x)  # make sure the line is long enough\n",
    "yy = a * xx - (classif.intercept_[0]) / w[1]\n",
    "\n",
    "\n",
    "cnt=3\n",
    "theme = px.colors.diverging.Portland\n",
    "\n",
    "used_columns = df.columns\n",
    "pal = sns.husl_palette(8, s=.45)\n",
    "lut = dict(zip(map(str, used_columns), pal))\n",
    "colors = pd.Series(used_columns, index=df.columns).map(lut)\n",
    "\n",
    "for i,ttt in enumerate(traces):\n",
    "    #if cnt==len(theme):\n",
    "    #    cnt=0\n",
    "    size = 6\n",
    "    if i>1:\n",
    "        pass\n",
    "        #size = 12\n",
    "    else:\n",
    "        size = 6\n",
    "        text = df[df.index.isin(ttt[3])].index\n",
    "  \n",
    "\n",
    "    \n",
    "    if ttt==None:\n",
    "        trace = {\n",
    "          \"line\": {\n",
    "            \"dash\": \"solid\", \n",
    "            \"color\": \"rgb(255,0,0)\", \n",
    "            \"shape\": \"linear\", \n",
    "            \"width\": 2\n",
    "          }, \n",
    "          \"mode\": \"lines\", \n",
    "          \"name\": \"Decision Boundary\", \n",
    "          \"text\": \"Decision Boundary\", \n",
    "          \"type\": \"scatter\", \n",
    "          \"x\": xx, \n",
    "          \"y\": yy,\n",
    "          \"yaxis\": \"y1\", \n",
    "          \"showlegend\": False\n",
    "        }\n",
    "    else:\n",
    "        trace = dict(\n",
    "        type='scatter',\n",
    "        text=text,\n",
    "        x=ttt[0],\n",
    "        y=ttt[1],\n",
    "        mode='markers',\n",
    "        name=ttt[2],\n",
    "        marker=dict(\n",
    "            color=colors[int(cnt*2.5)],\n",
    "            size=size,\n",
    "            line=dict(\n",
    "                color='rgba(217, 217, 217, 0.14)',\n",
    "                width=0.5),\n",
    "            opacity=0.8)\n",
    "        )\n",
    "    cnt+=1\n",
    "    data.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout_title_text=\"steady_state_voltage_stimend_3.0x versus AP1DelayMeanTest_3.0x\"\n",
    ")#,\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=1050\n",
    ")\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    pio.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbp_idx_labels\n",
    "text = df[df.index.isin(bbp_idx_labels)].index\n",
    "text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a TSNE embedding in two dimensions\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "tsne.fit(df.values)\n",
    "x = tsne.embedding_.T\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(x[0,experiment_idx_labels],x[1,experiment_idx_labels],c='blue',cmap='rainbow',label='data')\n",
    "plt.scatter(x[0,model_index_labels],x[1,model_index_labels],c='red',cmap='rainbow',label='models')\n",
    "plt.scatter(x[0,new_model_labels],x[1,new_model_labels],c='green',cmap='rainbow',label='optimized models')\n",
    "plt.scatter(x[0,gouwens_idx_labels],x[1,gouwens_idx_labels],c='black',cmap='rainbow',label='Gouwens models')\n",
    "plt.scatter(x[0,markram_idx_labels],x[1,markram_idx_labels],c='orange',s=100, cmap='rainbow',label='Markram models')\n",
    "\n",
    "plt.scatter(x[0,experiment_idx_labels][42],x[1,experiment_idx_labels][42],s=700,c='purple', alpha=0.3,cmap='rainbow',label='Layer 4 aspiny 313862167')\n",
    "\n",
    "plt.scatter(x[0,gouwens_idx_labels][-7],x[1,gouwens_idx_labels][-7],s=700,c='purple', alpha=0.3,cmap='rainbow',label=' layer 4 spiny 479728896')\n",
    "plt.scatter(x[0,bbp],x[1,bbp],s=90, c='purple',cmap='rainbow',label='BBP')\n",
    "\n",
    "#except:\n",
    "#    pass\n",
    "legend = ax.legend()#handles, labels, loc=\"upper right\", title=\"Sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cluster grams for two things. \n",
    "\n",
    "1. which cells are clustered with other cells?\n",
    "\n",
    "2. which features are clustered with other features?\n",
    "\n",
    "Below is\n",
    "1. which cells are clustered with other cells?\n",
    "\n",
    "Unfortunately this is hard to understand, it may be possible to truncate the dendrogram tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(data=df.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is:\n",
    "2. which features are clustered with other features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now lets look at how clustered the tests are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_columns = df.columns\n",
    "# Create a categorical palette to identify the networks\n",
    "pal = sns.husl_palette(8, s=.45)\n",
    "lut = dict(zip(map(str, used_columns), pal))\n",
    "\n",
    "# Convert the palette to vectors that will be drawn on the side of the matrix\n",
    "#networks = df.columns.get_level_values(\"network\")\n",
    "colors = pd.Series(used_columns, index=df.columns).map(lut)\n",
    "\n",
    "# Draw the full plot\n",
    "sns.clustermap(df.corr(), center=0, cmap=\"vlag\",\n",
    "               row_colors=colors, col_colors=colors,\n",
    "               linewidths=.75, figsize=(13, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.T.corr()\n",
    "temp;\n",
    "sns.clustermap(temp, figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.figure_factory import create_dendrogram\n",
    "df_ = pd.DataFrame(temp)\n",
    "labels=df_.index\n",
    "labels\n",
    "#fig = ff.create_dendrogram(X, orientation='left', labels=names)\n",
    "fig = create_dendrogram(df_,labels=df_.index, orientation='left')\n",
    "fig.update_layout(width=800, height=8000)\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    pio.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.figure_factory import create_dendrogram\n",
    "df_ = pd.DataFrame(iso.T)\n",
    "fig = create_dendrogram(df_)\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    pio.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[bbp];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "data = []\n",
    "trace0=(x[0,experiment_idx_labels],x[1,experiment_idx_labels],'Allen Brain Ephys Data',experiment_idx)\n",
    "trace1=(x[0,model_index_labels],x[1,model_index_labels],'models',model_idx)\n",
    "trace2=(x[0,new_model_labels],x[1,new_model_labels],'optimized models',new_models_idx)\n",
    "trace3=(x[0,gouwens_idx_labels],x[1,gouwens_idx_labels],'Gouwens models',gouwens_idx)\n",
    "trace4=(x[0,markram_idx_labels],x[1,markram_idx_labels],'Markram models',markram_idx)\n",
    "trace7=(x[0,bbp],x[1,bbp],'Blue Brain Project Ephys Data',bbp_idx_labels)\n",
    "\n",
    "trace5=([x[0,experiment_idx_labels][42]],[x[1,experiment_idx_labels]],'Layer 4 aspiny 313862167',experiment_idx_labels)\n",
    "trace6=([x[0,gouwens_idx_labels][-7]],[x[1,gouwens_idx_labels][-7]],'Layer 4 spiny 479728896',gouwens_idx_labels)\n",
    "\n",
    "# trace7=([iso[0,gouwens_idx_labels][-7]],[iso[1,gouwens_idx_labels][-7]],'Layer 4 spiny 479728896',gouwens_idx_labels)\n",
    "#plt.scatter(iso[0,bbp_labels],iso[1,bbp_labels],s=10, c='purple',cmap='rainbow',label='BBP')\n",
    "trace7=(x[0,bbp],x[1,bbp],'Blue Brain Project Ephys Data',df[bbpindex].index.values)\n",
    "\n",
    "classif = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "classif.fit(x.T, groundtruth)\n",
    "min_x = np.min(x[0, :])\n",
    "max_x = np.max(x[0, :])\n",
    "w = classif.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(min_x, max_x)  # make sure the line is long enough\n",
    "yy = a * xx - (classif.intercept_[0]) / w[1]\n",
    "\n",
    "\n",
    "\n",
    "used_columns = df.columns\n",
    "pal = sns.husl_palette(8, s=.45)\n",
    "lut = dict(zip(map(str, used_columns), pal))\n",
    "colors = pd.Series(used_columns, index=df.columns).map(lut)\n",
    "\n",
    "\n",
    "traces = [trace0,trace1,trace2,trace3,trace4,trace5,trace6,trace7,None]\n",
    "cnt=0\n",
    "#theme = px.colors.diverging.Portland\n",
    "for i,ttt in enumerate(traces):\n",
    "    if cnt==len(theme):\n",
    "        cnt=0\n",
    "    if i>1 and i!=7:\n",
    "        size = 12\n",
    "    else:\n",
    "        size = 6\n",
    "        \n",
    "    if i==8:\n",
    "        trace = {\n",
    "          \"line\": {\n",
    "            \"dash\": \"solid\", \n",
    "            \"color\": \"rgb(255,0,0)\", \n",
    "            \"shape\": \"linear\", \n",
    "            \"width\": 2\n",
    "          }, \n",
    "          \"mode\": \"lines\", \n",
    "          \"name\": \"Decision Boundary\", \n",
    "          \"text\": \"Decision Boundary\", \n",
    "          \"type\": \"scatter\", \n",
    "          \"x\": xx, \n",
    "          \"y\": yy,\n",
    "          \"yaxis\": \"y1\", \n",
    "          \"showlegend\": False\n",
    "        }\n",
    "    else:\n",
    "        trace = dict(\n",
    "            type='scatter',\n",
    "            text = df[df.index.isin(ttt[3])].index,\n",
    "            x=ttt[0],\n",
    "            y=ttt[1],\n",
    "            mode='markers',\n",
    "            name=ttt[2],\n",
    "            marker=dict(\n",
    "                color=colors[cnt],\n",
    "                size=size,\n",
    "                line=dict(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5),\n",
    "                opacity=0.8)\n",
    "        )\n",
    "    data.append(trace)\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "layout = go.Layout(yaxis=dict(range=[-50, 50]))\n",
    "fig = go.Figure(\n",
    "    data=data,\n",
    "    layout_title_text=\"steady_state_voltage_stimend_3.0x versus AP1DelayMeanTest_3.0x\", layout=layout\n",
    ")\n",
    "\n",
    "#layout = {'scene': {'xaxis': {'showspikes': False}}}\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=1050\n",
    ")\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    pio.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE \n",
    "The TSNE plot does a better job of spatially sperating experimental data from theoretical models in dimension reduced Druckman feature space.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally we  examine the dimension that contributes the most to cluster seperation by looking at variance explained. This gives us an educated guess about dimensions that contribute the most weight to the axis of the PCA projection spaces plotted above.\n",
    "\n",
    "Note to self, experimental_index needs updtating to include BBP cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "df_models = df[~df.index.isin(experiment_idx)]\n",
    "df_data = df[df.index.isin(experiment_idx)]\n",
    "# Assume they have the same columns\n",
    "\n",
    "df_combined = pd.concat([df_data, df_models])\n",
    "\n",
    "groundtruth = np.array(df.index.isin(experiment_idx))\n",
    "rfc = RandomForestClassifier()\n",
    "X = df_combined.values\n",
    "rfc.fit(X, groundtruth)\n",
    "\n",
    "importances = pd.Series(index = df_combined.columns, data=rfc.feature_importances_)\n",
    "groundtruth[-9:-1]\n",
    "\n",
    "print(importances.sort_values(ascending=False)[0:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try:\n",
    "    df = df.drop(columns = ['ohmic_input_resistance_3.0x'])\n",
    "    df = df.drop(columns = ['ohmic_input_resistance_1.5x'])\n",
    "    df = df.drop(columns = ['InputResistanceTest_1.5x'])\n",
    "    df = df.drop(columns = ['InputResistanceTest_3.0x'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "df = df.drop(columns = ['ohmic_input_resistance_1.5x'])\n",
    "'''\n",
    "\n",
    "#ohmic_input_resistance_3.0x             0.387\n",
    "#sag_ratio1_3.0x                         0.151\n",
    "#ohmic_input_resistance_1.5x             0.104\n",
    "#AP1RateOfChangePeakToTroughTest_3.0x    0.070\n",
    "#InputResistanceTest_3.0x                0.054\n",
    "\n",
    "df_models = df[~df.index.isin(experiment_idx)]\n",
    "df_data = df[df.index.isin(experiment_idx)]\n",
    "# Assume they have the same columns\n",
    "\n",
    "df_combined = pd.concat([df_data, df_models])\n",
    "\n",
    "groundtruth = np.array(df.index.isin(experiment_idx))\n",
    "rfc = RandomForestClassifier()\n",
    "X = df_combined.values\n",
    "rfc.fit(X, groundtruth)\n",
    "\n",
    "importances = pd.Series(index = df_combined.columns, data=rfc.feature_importances_)\n",
    "groundtruth[-9:-1]\n",
    "\n",
    "print(importances.sort_values(ascending=False)[0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make four regions for ground truth\n",
    "for a region dependent classification with four regions (0,1,2,3)\n",
    "\n",
    "Gouwens and Allen 0\n",
    "Markram and BBP 1\n",
    "Hippocampus 2\n",
    "Thalamocortical 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "df_models = df[~df.index.isin(experiment_idx)]\n",
    "df_data = df[df.index.isin(experiment_idx)]\n",
    "# Assume they have the same columns\n",
    "\n",
    "df_combined = pd.concat([df_data, df_models])\n",
    "\n",
    "groundtruth = np.array(df.index.isin(experiment_idx))\n",
    "rfc = RandomForestClassifier()\n",
    "X = df_combined.values\n",
    "rfc.fit(X, groundtruth)\n",
    "\n",
    "importances = pd.Series(index = df_combined.columns, data=rfc.feature_importances_)\n",
    "groundtruth[-9:-1]\n",
    "\n",
    "print(importances.sort_values(ascending=False)[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('InputResistanceTest_3.0x' in df_data.columns)\n",
    "print('InputResistanceTest_3.0x' in df_models.columns)\n",
    "print('InputResistanceTest_3.0x' in df_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.scatter(df_data['InputResistanceTest_1.5x'], df_data['ohmic_input_resistance_3.0x'],label='experimental data')\n",
    "plt.scatter(df_models['InputResistanceTest_1.5x'], df_models['ohmic_input_resistance_3.0x'],label='models')\n",
    "\n",
    "plt.xlabel('InputResistanceTest_1.5x')\n",
    "plt.ylabel('ohmic_input_resistance_3.0x')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.scatter(df_data['sag_ratio1_3.0x'], df_data['ohmic_input_resistance_3.0x'],label='experimental data')\n",
    "plt.scatter(df_models['sag_ratio1_3.0x'], df_models['ohmic_input_resistance_3.0x'],label='models')\n",
    "\n",
    "plt.xlabel('sag_ratio1_3.0x')\n",
    "plt.ylabel('ohmic_input_resistance_3.0x')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "df_data_b = df[bbp]\n",
    "df_data_b[:] = preprocessing.normalize(df_data_b.values, norm='l2')\n",
    "experiment_df[:] = preprocessing.normalize(experiment_df.values, norm='l2')\n",
    "#temp_bbpindex = [i for i,idx in enumerate(df_data.index.values) if idx in stable_list]\n",
    "#temp_bbpindex\n",
    "\n",
    "df_data = pd.concat([experiment_df,df_data_b])\n",
    "#temp_allenindex = list(range(len(temp_bbpindex),len(df_data)))\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "plt.scatter(df_data_b['sag_ratio1_3.0x'], df_data_b['AP1RateOfChangePeakToTroughTest_3.0x'],label='BBP experimental Cells')\n",
    "plt.scatter(experiment_df['sag_ratio1_3.0x'], experiment_df['AP1RateOfChangePeakToTroughTest_3.0x'],label='Allen experimental Cells')\n",
    "#plt.scatter(df_o_m['steady_state_voltage_stimend_3.0x'], df_o_m['AP1DelayMeanTest_3.0x'],label='models')\n",
    "\n",
    "plt.xlabel('sag_ratio1_3.0x')\n",
    "plt.ylabel('AP1RateOfChangePeakToTroughTest_3.0x')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL output is imported as a dataframe variable called 'df'\n",
    "#import pandas as pd\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# 3D scatter plot. Resource: https://plot.ly/python/3d-scatter-plots/\n",
    "theme = px.colors.diverging.Portland\n",
    "\n",
    "experimental_data = go.Scatter3d(\n",
    "    x = df_data['sag_ratio1_3.0x'],\n",
    "    y = df_data['AP1RateOfChangePeakToTroughTest_3.0x'],\n",
    "    z = df_data['ohmic_input_resistance_3.0x'],\n",
    "    mode ='markers',\n",
    "    name = 'experimental data',\n",
    "    marker =dict(\n",
    "      color = theme[0],\n",
    "      size = 3,\n",
    "      opacity = 0.9\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "opt_models = go.Scatter3d(\n",
    "    x = df_o_m3['AP_begin_voltage_1.5x'],\n",
    "    y = df_o_m3['AP1AHPDepthTest_1.5x'],\n",
    "    z = df_o_m3['decay_time_constant_after_stim_3.0x'],\n",
    "    mode ='markers',\n",
    "    name = 'bad Model',\n",
    "    marker =dict(\n",
    "      color = theme[1],\n",
    "      size = 3,\n",
    "      opacity = 0.9\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "models = go.Scatter3d(\n",
    "    x = experiment_df['sag_ratio1_3.0x'],\n",
    "    y = experiment_df['AP1RateOfChangePeakToTroughTest_3.0x'],\n",
    "    z = experiment_df['ohmic_input_resistance_3.0x'],\n",
    "    mode ='markers',\n",
    "    name = 'Models',\n",
    "    marker =dict(\n",
    "      color = theme[1],\n",
    "      size = 3,\n",
    "      opacity = 0.9\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [experimental_data , models]\n",
    "layout = go.Layout(\n",
    "   scene = dict(xaxis = dict(title='AP_begin_voltage_1.5x'),\n",
    "                yaxis = dict(title='AP1AHPDepthTest_1.5x'),\n",
    "                zaxis = dict(title='decay_time_constant_after_stim_3.0x'),),\n",
    "    margin=dict(\n",
    "        l=10,\n",
    "        r=10,\n",
    "        b=10,\n",
    "        t=10\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "df_o_m3\n",
    "# Use Periscope to visualize a dataframe, text, or an image by passing data to periscope.table(), periscope.text(), or periscope.image() respectively.\n",
    "#py.show(fig)\n",
    "#py.plot(fig, filename='data_only_3D_scatter.html') \n",
    "\n",
    "\n",
    "if GITHUB:\n",
    "    fig.show(\"svg\")\n",
    "else:\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I wrote above Random Forest Variance explained, tells us the dimensions of the Druckman feature space that most strongly assist in classifying models versus data. When we identify features that seperate models and data using Variance Explained, we are then able to iteratively variables that contribute more heavily to data variance. We can remove variables that explain most variance, until machine classification can no longer tell models and data apart, leaving us with a small list of tests which models need to perform better on, these tests correspond to measurable electrical properties of cells that need to be better aligned with data.\n",
    "\n",
    "Two such measurable electrical properties are Druckman features with high variance explained are re: \"Input Resistance\" in models and data, as well as \"AP2RateOfChangePeakToTroughTest\". This second dimension called AP2RateOfChangePeakToTroughTest means: when considering multi-spiking waveforms observed, at the second Action Potential/Spike, how rapid is the average decay from peak to trough of the second AP wave. Since Action Potential wave attack and decay shapes are non-linear, the instantaneous gradient from the peak of the wave is not informative, and it is more useful to measure the time interval needed needed for a decay from a spike, to a state of hyperpolarization, corresponding to a neurons \"refractory-period\".\n",
    "\n",
    "Already, we are have arrived at useful information, pertaining to the point of the exercise, as we now have a small list of electrical tests, that we want optimized models to perform better on, such that models and data will be more aligned with each other.\n",
    "\n",
    "As neural modelers with a great interest in mimicing a diverse range of experimental data using models. The least convincing aspects of our models as mimics of data, are these top ten features. In other words the least convincing aspects of our models are: AP2RateOfChangePeakToTroughTest, Input Resistance values (a scalar), and the amplitude of the first and second spike.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Results When I Use Random Forests on all 51 feaature dimensions\n",
    "* remember that our ground truth labels are booleans that are defined like this:\n",
    "groundtruth = np.array(df_combined.index.isin(experiment_idx))\n",
    "Which is labeled as \"True\" for this data point is an experiment, and \"False\" for this data point is a model.\n",
    "Machine classification can successfuly discrimate that our optimized cells are models and not data (this is bad news for us).\n",
    "\n",
    "In this context in order to bolster out optimized models, a high ***false-negative*** rate is desirable. Unfortunately for us, that is not what we see. The Random Forest Classifier (RFC) correctly identies that all 11 of the new optimized cells are not derived from experiments (they are models). That is bad news for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m.columns;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc = RandomForestClassifier()\n",
    "X = df_combined.values\n",
    "X;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_m.values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > 99%\n",
    "of models are classified as models when optimized models are not included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the random forest is trained on the optimized models too\n",
    "Then the area under the ROC curve becomes closer to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we feed in the 7 new optimized models as \"validation data\", in this context, the RFC is still okay at classifying our new models correctly as models, and not data. However, the RFC performance is significanlty worse, as the optimized cells have tricked the RFC ***4*** times.\n",
    "\n",
    "Also since the output of the TSNE-PCA varies with each run, as it is seeded with a psuedo random numnber generator, the projection space that the RFC acts on is different each time. Meaning that the ***FPR*** and the ***TPR*** rates vary slightly on each run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the plot below we show the we show the decision boundary as used by our classifier.\n",
    "\n",
    "The decision boundary llows us to see if the newer optimized models are classified as data or models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Blue means experimental data, red means model, as machine categorized using Sklearns Random Forest classifier.\n",
    "\n",
    "A general trend is apparent at the macro scale. It seems as if the bottom 2/3rds of the plane belong to data, and the remaining upper 1/3rd of the plane belongs to models, however you can also see on a micro scale there are lots of small pockets, or islands of model decision territory inside, what is more generally regarded as data territory.\n",
    "\n",
    "Although the large red dots, appear in the correct side of the macro decision boundary, zooming in would reveal that these optimized models are in fact enveloped by model island that is excatly small enough to contain them. Therefore random forest classification, correctly classifies the optimized models as models.\n",
    "\n",
    "# The above figure allows us\n",
    "To argue that the newer optimized models are closer to cluster centres and often fall on the experimental data side of the decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets switch back to using all 38 features to classify\n",
    "Only as its easier for me to debug, and I can make progress more quickly.\n",
    "Using cross validation below you can see that this approach is generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --update sklearn\n",
    "len(groundtruth)\n",
    "groundtruth\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_combined.values, groundtruth, test_size=0.5, random_state=42)    \n",
    "\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All 7 seven new models are classified as data\n",
    "\n",
    "Ie it is false that they are percieved as models, its true that they are percieved as data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will enable us to use a cross-validation Approach.\n",
    "Cross-validation will help us to check the generalizability of our model, by better navigating the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on misclassification.\n",
    "Even though RFC can be over-fit by using all the data over 148 features a False negative is still sometimes reported.\n",
    "Experimental data point with identifier: \"482764620\" is sometimes falsely classified as a model, but we know from ground truth that it is an experiment.\n",
    "\n",
    "The outputs of thistest are a bit different each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat above with just experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model dataframe\n",
    "model_idx = [idx for idx in df.index.values if type(idx)==str]\n",
    "model_no_trans_df = df[df.index.isin(model_idx)]\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "experiment_no_trans_df = df[df.index.isin(experiment_idx)]\n",
    "experiment_df = experiment_no_trans_df.copy()\n",
    "experiment_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[:] = ss.fit_transform(model_no_trans_df.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
